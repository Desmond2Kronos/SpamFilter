{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python: {}\".format(sys.version))\n",
    "print(\"NLTK: {}\".format(nltk.__version__))\n",
    "print(\"Sklearn: {}\".format(sklearn.__version__))\n",
    "print(\"pandas: {}\".format(pandas.__version__))\n",
    "print(\"numpy: {}\".format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the sms\n",
    "df = pd.read_table('SMSSpamCollection', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print useful data\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the class distribution\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class labels to binary values 0=ham, 1=spam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "print(classes[:10])\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the sms message data\n",
    "text_messages = df[1]\n",
    "print(text_messages[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expression to replace email, urls,\n",
    "\n",
    "# replace email address with 'emailaddr'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$', 'emailaddr')\n",
    "\n",
    "# replace urls with 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}{/\\S*}?$', 'webaddress')\n",
    "\n",
    "# replace money symbols with 'moneysymb'\n",
    "processed = processed.str.replace(r'ï¿½|\\$','moneysymb')\n",
    "\n",
    "# replcae 10 digit phone numbers with 'phone number'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumber')\n",
    "\n",
    "# replace normal numbers with numbr\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace punc\n",
    "processed = processed.str.replace(r'[\\n\\d\\s]', ' ')\n",
    "\n",
    "#replace whitespace\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# remove leading and trailing whitespace\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the words to lowercase\n",
    "processed = processed.str.lower()\n",
    "print(processed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# creating a bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print total no of words and print most common\n",
    "print(\"Number of words:: {}\".format(len(all_words)))\n",
    "print(\"Most common words:: {}\".format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define find_features function\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find features for all messages\n",
    "messages = zip(processed, Y)\n",
    "\n",
    "# define a seed for reproducability\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "#np.random.shuffle(messages)\n",
    "\n",
    "# call find_features for each sms messages\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data sets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size=0.25, random_state= seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training {}'.format(len(training)))\n",
    "print('Testing {}'.format(len(testing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scikit-Learn Classifiers with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model to train\n",
    "names = ['K Nearest Neighbors', 'Decision Tree',\n",
    "        'Random Forest', 'Logistic Regression',\n",
    "        'SGD Classifier', 'Naive Bayes', \n",
    "        'SVN Linear']\n",
    "\n",
    "classifier = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter=100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel='linear')\n",
    "]\n",
    "#models = zip(names, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors: Accuracy: 93.3237616654702\n",
      "Decision Tree: Accuracy: 97.20028715003589\n",
      "Random Forest: Accuracy: 98.06173725771716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kapil Pawar\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Accuracy: 98.7078248384781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kapil Pawar\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier: Accuracy: 97.98994974874373\n",
      "Naive Bayes: Accuracy: 98.34888729361091\n",
      "SVN Linear: Accuracy: 98.56424982053123\n"
     ]
    }
   ],
   "source": [
    "# wrap models in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "for (name, model) in zip(names, classifier):\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing) * 100\n",
    "    print('{}: Accuracy: {}'.format(name, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensamble method - Voting Classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = zip(names, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensamble Method Accuracy: 98.63603732950466\n"
     ]
    }
   ],
   "source": [
    "nltk_ensamble = SklearnClassifier(VotingClassifier(estimators=list(zip(names, classifier)), voting='hard', n_jobs=-1))\n",
    "nltk_ensamble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensamble, testing) * 100\n",
    "print(\"Ensamble Method Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
